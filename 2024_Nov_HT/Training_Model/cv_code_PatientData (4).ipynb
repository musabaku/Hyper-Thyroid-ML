{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2e7a88-1be8-408a-a4d8-98d51ac68466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Files: []\n",
      "Mask Files: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat0_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat12_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat14_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat17_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat20_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat25_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat50_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat55_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\pat59_cropped_seg.nii']\n",
      "Number of images: 0\n",
      "Number of masks: 9\n",
      "Dataset successfully organized!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Base directory paths\n",
    "BASE_DIR = r\"C:\\Users\\ISU\\cv_dataset\\os_dataset\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "\n",
    "# Ensure all directories exist\n",
    "os.makedirs(os.path.join(TRAIN_DIR, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TRAIN_DIR, \"masks\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(VAL_DIR, \"images\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(VAL_DIR, \"masks\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TEST_DIR, \"images\"), exist_ok=True)\n",
    "\n",
    "# Define patterns to locate files (focus on *_cropped.nii and *_seg.nii files)\n",
    "image_pattern = \"*_cropped.nii\"  # Only the image files, without the *_seg_endpoints\n",
    "mask_pattern = \"*_seg.nii\"  # Only the mask files (exclude *_seg_endpoints)\n",
    "\n",
    "# Unzipped files\n",
    "image_files = sorted(glob.glob(os.path.join(BASE_DIR, image_pattern)))\n",
    "mask_files = sorted(glob.glob(os.path.join(BASE_DIR, mask_pattern)))\n",
    "\n",
    "# Debugging: Print found files\n",
    "print(\"Image Files:\", image_files)\n",
    "print(\"Mask Files:\", mask_files)\n",
    "print(\"Number of images:\", len(image_files))\n",
    "print(\"Number of masks:\", len(mask_files))\n",
    "\n",
    "# Ensure the number of images matches masks\n",
    "assert len(image_files) == len(mask_files), \"Mismatch between image and mask files.\"\n",
    "\n",
    "# Split dataset into train, val, and test\n",
    "train_img, temp_img, train_mask, temp_mask = train_test_split(image_files, mask_files, test_size=0.3, random_state=42)\n",
    "val_img, test_img, val_mask, test_mask = train_test_split(temp_img, temp_mask, test_size=0.5, random_state=42)\n",
    "\n",
    "# Helper function to move files\n",
    "def move_files(image_list, mask_list, dest_dir, for_test=False):\n",
    "    if not for_test:\n",
    "        for img, mask in zip(image_list, mask_list):\n",
    "            shutil.move(img, os.path.join(dest_dir, \"images\", os.path.basename(img)))\n",
    "            shutil.move(mask, os.path.join(dest_dir, \"masks\", os.path.basename(mask)))\n",
    "    else:\n",
    "        for img in image_list:\n",
    "            shutil.move(img, os.path.join(dest_dir, \"images\", os.path.basename(img)))\n",
    "\n",
    "# Organize files into train, val, and test folders\n",
    "move_files(train_img, train_mask, TRAIN_DIR)\n",
    "move_files(val_img, val_mask, VAL_DIR)\n",
    "move_files(test_img, [], TEST_DIR, for_test=True)\n",
    "\n",
    "print(\"Dataset successfully organized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b88374e-f60b-4985-8f6d-2cdf31e18d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Directory: C:\\Users\\ISU\\cv_dataset\\os_dataset\n",
      "Training Directory: C:\\Users\\ISU\\cv_dataset\\os_dataset\\train\n",
      "Validation Directory: C:\\Users\\ISU\\cv_dataset\\os_dataset\\val\n",
      "Test Directory: C:\\Users\\ISU\\cv_dataset\\os_dataset\\test\n",
      "Training Images Paths: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat10_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat11_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat13_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat16_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat18_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat19_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat1_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat22_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat23_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat24_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat26_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat27_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat28_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat29_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat2_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat30_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat31_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat32_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat33_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat34_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat35_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat36_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat37_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat39_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat40_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat41_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat43_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat44_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat45_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat46_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat47_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat48_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat49_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat4_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat52_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat54_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat56_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat58_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat5_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat6_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat8_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\images\\\\pat9_cropped.nii']\n",
      "Training Masks Paths: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat10_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat11_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat13_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat16_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat18_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat19_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat1_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat22_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat23_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat24_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat26_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat27_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat28_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat29_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat2_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat30_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat31_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat32_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat33_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat34_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat35_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat36_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat37_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat39_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat40_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat41_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat43_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat44_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat45_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat46_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat47_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat48_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat49_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat4_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat52_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat54_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat56_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat58_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat5_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat6_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat8_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\train\\\\masks\\\\pat9_cropped_seg.nii']\n",
      "Validation Images Paths: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat15_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat21_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat38_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat3_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat42_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat51_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat53_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat57_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\images\\\\pat7_cropped.nii']\n",
      "Validation Masks Paths: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat15_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat21_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat38_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat3_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat42_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat51_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat53_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat57_cropped_seg.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\val\\\\masks\\\\pat7_cropped_seg.nii']\n",
      "Testing Images Paths: ['C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat0_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat12_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat14_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat17_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat20_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat25_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat50_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat55_cropped.nii', 'C:\\\\Users\\\\ISU\\\\cv_dataset\\\\os_dataset\\\\test\\\\images\\\\pat59_cropped.nii']\n",
      "Number of training images found: 42\n",
      "Number of training masks found: 42\n",
      "Number of validation images found: 9\n",
      "Number of validation masks found: 9\n",
      "Number of testing images found: 9\n",
      "Contents of C:\\Users\\ISU\\cv_dataset\\os_dataset\\train/images:  ['pat10_cropped.nii', 'pat11_cropped.nii', 'pat13_cropped.nii', 'pat16_cropped.nii', 'pat18_cropped.nii', 'pat19_cropped.nii', 'pat1_cropped.nii', 'pat22_cropped.nii', 'pat23_cropped.nii', 'pat24_cropped.nii', 'pat26_cropped.nii', 'pat27_cropped.nii', 'pat28_cropped.nii', 'pat29_cropped.nii', 'pat2_cropped.nii', 'pat30_cropped.nii', 'pat31_cropped.nii', 'pat32_cropped.nii', 'pat33_cropped.nii', 'pat34_cropped.nii', 'pat35_cropped.nii', 'pat36_cropped.nii', 'pat37_cropped.nii', 'pat39_cropped.nii', 'pat40_cropped.nii', 'pat41_cropped.nii', 'pat43_cropped.nii', 'pat44_cropped.nii', 'pat45_cropped.nii', 'pat46_cropped.nii', 'pat47_cropped.nii', 'pat48_cropped.nii', 'pat49_cropped.nii', 'pat4_cropped.nii', 'pat52_cropped.nii', 'pat54_cropped.nii', 'pat56_cropped.nii', 'pat58_cropped.nii', 'pat5_cropped.nii', 'pat6_cropped.nii', 'pat8_cropped.nii', 'pat9_cropped.nii']\n",
      "Contents of C:\\Users\\ISU\\cv_dataset\\os_dataset\\train/masks:  ['pat10_cropped_seg.nii', 'pat11_cropped_seg.nii', 'pat13_cropped_seg.nii', 'pat16_cropped_seg.nii', 'pat18_cropped_seg.nii', 'pat19_cropped_seg.nii', 'pat1_cropped_seg.nii', 'pat22_cropped_seg.nii', 'pat23_cropped_seg.nii', 'pat24_cropped_seg.nii', 'pat26_cropped_seg.nii', 'pat27_cropped_seg.nii', 'pat28_cropped_seg.nii', 'pat29_cropped_seg.nii', 'pat2_cropped_seg.nii', 'pat30_cropped_seg.nii', 'pat31_cropped_seg.nii', 'pat32_cropped_seg.nii', 'pat33_cropped_seg.nii', 'pat34_cropped_seg.nii', 'pat35_cropped_seg.nii', 'pat36_cropped_seg.nii', 'pat37_cropped_seg.nii', 'pat39_cropped_seg.nii', 'pat40_cropped_seg.nii', 'pat41_cropped_seg.nii', 'pat43_cropped_seg.nii', 'pat44_cropped_seg.nii', 'pat45_cropped_seg.nii', 'pat46_cropped_seg.nii', 'pat47_cropped_seg.nii', 'pat48_cropped_seg.nii', 'pat49_cropped_seg.nii', 'pat4_cropped_seg.nii', 'pat52_cropped_seg.nii', 'pat54_cropped_seg.nii', 'pat56_cropped_seg.nii', 'pat58_cropped_seg.nii', 'pat5_cropped_seg.nii', 'pat6_cropped_seg.nii', 'pat8_cropped_seg.nii', 'pat9_cropped_seg.nii']\n",
      "Contents of C:\\Users\\ISU\\cv_dataset\\os_dataset\\val/images:  ['pat15_cropped.nii', 'pat21_cropped.nii', 'pat38_cropped.nii', 'pat3_cropped.nii', 'pat42_cropped.nii', 'pat51_cropped.nii', 'pat53_cropped.nii', 'pat57_cropped.nii', 'pat7_cropped.nii']\n",
      "Contents of C:\\Users\\ISU\\cv_dataset\\os_dataset\\val/masks:  ['pat15_cropped_seg.nii', 'pat21_cropped_seg.nii', 'pat38_cropped_seg.nii', 'pat3_cropped_seg.nii', 'pat42_cropped_seg.nii', 'pat51_cropped_seg.nii', 'pat53_cropped_seg.nii', 'pat57_cropped_seg.nii', 'pat7_cropped_seg.nii']\n",
      "Contents of C:\\Users\\ISU\\cv_dataset\\os_dataset\\test/images:  ['pat0_cropped.nii', 'pat12_cropped.nii', 'pat14_cropped.nii', 'pat17_cropped.nii', 'pat20_cropped.nii', 'pat25_cropped.nii', 'pat50_cropped.nii', 'pat55_cropped.nii', 'pat59_cropped.nii']\n",
      "Test image shape: (192, 269, 190)\n",
      "Training dataset size: 42\n",
      "Validation dataset size: 9\n",
      "Test dataset size: 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "\n",
    "# Base directories (already set in your previous code)\n",
    "BASE_DIR = r\"C:\\Users\\ISU\\cv_dataset\\os_dataset\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "\n",
    "# Print the base and dataset directory paths\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Training Directory: {TRAIN_DIR}\")\n",
    "print(f\"Validation Directory: {VAL_DIR}\")\n",
    "print(f\"Test Directory: {TEST_DIR}\")\n",
    "\n",
    "# Get all image and mask paths for training, validation, and testing\n",
    "train_img_paths = glob.glob(os.path.join(TRAIN_DIR, \"images\", \"*.nii\"))  # Change to *.nii\n",
    "train_mask_paths = glob.glob(os.path.join(TRAIN_DIR, \"masks\", \"*.nii\"))  # Change to *.nii\n",
    "val_img_paths = glob.glob(os.path.join(VAL_DIR, \"images\", \"*.nii\"))  # Change to *.nii\n",
    "val_mask_paths = glob.glob(os.path.join(VAL_DIR, \"masks\", \"*.nii\"))  # Change to *.nii\n",
    "test_img_paths = glob.glob(os.path.join(TEST_DIR, \"images\", \"*.nii\"))  # Change to *.nii\n",
    "\n",
    "# Debug: Print out the found image paths to verify if glob is working correctly\n",
    "print(\"Training Images Paths:\", train_img_paths)\n",
    "print(\"Training Masks Paths:\", train_mask_paths)\n",
    "print(\"Validation Images Paths:\", val_img_paths)\n",
    "print(\"Validation Masks Paths:\", val_mask_paths)\n",
    "print(\"Testing Images Paths:\", test_img_paths)\n",
    "\n",
    "# Check the number of files in each dataset and display the results\n",
    "print(f\"Number of training images found: {len(train_img_paths)}\")\n",
    "print(f\"Number of training masks found: {len(train_mask_paths)}\")\n",
    "print(f\"Number of validation images found: {len(val_img_paths)}\")\n",
    "print(f\"Number of validation masks found: {len(val_mask_paths)}\")\n",
    "print(f\"Number of testing images found: {len(test_img_paths)}\")\n",
    "\n",
    "# Check directory contents for the images and masks to confirm the files are in place\n",
    "print(f\"Contents of {TRAIN_DIR}/images: \", os.listdir(os.path.join(TRAIN_DIR, \"images\")))\n",
    "print(f\"Contents of {TRAIN_DIR}/masks: \", os.listdir(os.path.join(TRAIN_DIR, \"masks\")))\n",
    "print(f\"Contents of {VAL_DIR}/images: \", os.listdir(os.path.join(VAL_DIR, \"images\")))\n",
    "print(f\"Contents of {VAL_DIR}/masks: \", os.listdir(os.path.join(VAL_DIR, \"masks\")))\n",
    "print(f\"Contents of {TEST_DIR}/images: \", os.listdir(os.path.join(TEST_DIR, \"images\")))\n",
    "\n",
    "# Verify the accessibility of a sample image by loading it with nibabel\n",
    "if train_img_paths:\n",
    "    img_test = nib.load(train_img_paths[0])\n",
    "    print(f\"Test image shape: {img_test.shape}\")\n",
    "else:\n",
    "    print(\"No training images found for test loading.\")\n",
    "\n",
    "# Create Dataset instances (this will only work if images and masks are found)\n",
    "if len(train_img_paths) > 0 and len(train_mask_paths) > 0:\n",
    "    train_dataset = HeartDataset(train_img_paths, train_mask_paths)\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "else:\n",
    "    print(\"Training dataset is empty.\")\n",
    "\n",
    "if len(val_img_paths) > 0 and len(val_mask_paths) > 0:\n",
    "    val_dataset = HeartDataset(val_img_paths, val_mask_paths)\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "else:\n",
    "    print(\"Validation dataset is empty.\")\n",
    "\n",
    "if len(test_img_paths) > 0:\n",
    "    test_dataset = HeartDataset(test_img_paths)\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "else:\n",
    "    print(\"Test dataset is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1856b6b3-40f3-4818-9922-00f81c7c7448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 13.9252\n",
      "Epoch 2/5, Loss: 10.7028\n",
      "Epoch 3/5, Loss: 7.7665\n",
      "Epoch 4/5, Loss: 5.3920\n",
      "Epoch 5/5, Loss: 4.6838\n",
      "Prediction saved to: C:\\Users\\ISU\\cv_dataset\\patient_data\\segmented_mask.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import scipy.ndimage\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define configuration parameters\n",
    "INPUT_SHAPE = (128, 128, 64)  # The target shape after resizing\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Base directories (already set in your previous code)\n",
    "BASE_DIR = r\"C:\\Users\\ISU\\cv_dataset\\os_dataset\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "# No need for the test directory since we are using a single patient image for testing\n",
    "PATIENT_IMAGE_PATH = r\"C:\\Users\\ISU\\cv_dataset\\patient_data\\preprocessed_patient_image.nii.gz\"\n",
    "OUTPUT_MASK_PATH = r\"C:\\Users\\ISU\\cv_dataset\\patient_data\\segmented_mask.nii.gz\"\n",
    "\n",
    "# Create Dataset class to load images and masks\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def resize_3d(self, img, shape):\n",
    "        zoom_factors = [shape[i] / img.shape[i] for i in range(3)]\n",
    "        return scipy.ndimage.zoom(img, zoom_factors, order=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.image_paths[idx]).get_fdata()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize the image\n",
    "        img = self.resize_3d(img, INPUT_SHAPE)\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mask_paths:\n",
    "            mask = nib.load(self.mask_paths[idx]).get_fdata()\n",
    "            mask = self.resize_3d(mask, INPUT_SHAPE)\n",
    "            \n",
    "            # Convert mask to binary (0 or 1)\n",
    "            mask = (mask > 0).astype(np.float32)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "            \n",
    "            return img, mask\n",
    "        return img\n",
    "\n",
    "# Get all image and mask paths for training and validation\n",
    "train_img_paths = glob.glob(os.path.join(TRAIN_DIR, \"images\", \"*.nii\"))  # Change to *.nii\n",
    "train_mask_paths = glob.glob(os.path.join(TRAIN_DIR, \"masks\", \"*.nii\"))  # Change to *.nii\n",
    "val_img_paths = glob.glob(os.path.join(VAL_DIR, \"images\", \"*.nii\"))  # Change to *.nii\n",
    "val_mask_paths = glob.glob(os.path.join(VAL_DIR, \"masks\", \"*.nii\"))  # Change to *.nii\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = HeartDataset(train_img_paths, train_mask_paths)\n",
    "val_dataset = HeartDataset(val_img_paths, val_mask_paths)\n",
    "\n",
    "# Create DataLoader objects for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define and train the model (UNet)\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.upconv = nn.ConvTranspose3d(64, 64, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.upconv(enc)\n",
    "        dec = self.decoder(dec)\n",
    "        return dec\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = UNet3D(in_channels=1, out_channels=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), os.path.join(BASE_DIR, \"unet_heart_segmentation_3d.pth\"))\n",
    "\n",
    "# --- Inference on Patient Image ---\n",
    "def predict_patient_image(model, patient_image_path, output_mask_path, device):\n",
    "    model.eval()\n",
    "    # Load and preprocess patient image\n",
    "    img = nib.load(patient_image_path).get_fdata()\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize the image\n",
    "    img = scipy.ndimage.zoom(img, [float(dim) / img.shape[i] for i, dim in enumerate(INPUT_SHAPE)])\n",
    "    img = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)  # Add channel dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict segmentation mask\n",
    "        pred = model(img)\n",
    "        pred = (pred > 0.5).float().cpu().numpy().squeeze()  # Apply threshold\n",
    "\n",
    "    # Save the predicted mask as a NIfTI image\n",
    "    pred_img = nib.Nifti1Image(pred, np.eye(4))\n",
    "    nib.save(pred_img, output_mask_path)\n",
    "    print(f\"Prediction saved to: {output_mask_path}\")\n",
    "\n",
    "# Predict on the single patient image\n",
    "predict_patient_image(model, PATIENT_IMAGE_PATH, OUTPUT_MASK_PATH, DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f77133-0c46-4297-80e2-dbd5f85b90d8",
   "metadata": {},
   "source": [
    "# Latest code trying with slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3570bbb2-4de1-43e8-9a40-48d3d66b44c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 13.6793\n",
      "Epoch 2/20, Loss: 9.9353\n",
      "Epoch 3/20, Loss: 6.0051\n",
      "Epoch 4/20, Loss: 4.8690\n",
      "Epoch 5/20, Loss: 4.7674\n",
      "Epoch 6/20, Loss: 4.5850\n",
      "Epoch 7/20, Loss: 4.5716\n",
      "Epoch 8/20, Loss: 4.5396\n",
      "Epoch 9/20, Loss: 4.7467\n",
      "Epoch 10/20, Loss: 4.4343\n",
      "Epoch 11/20, Loss: 4.4589\n",
      "Epoch 12/20, Loss: 4.3445\n",
      "Epoch 13/20, Loss: 4.3669\n",
      "Epoch 14/20, Loss: 4.3203\n",
      "Epoch 15/20, Loss: 4.3363\n",
      "Epoch 16/20, Loss: 4.3019\n",
      "Epoch 17/20, Loss: 4.2591\n",
      "Epoch 18/20, Loss: 4.2115\n",
      "Epoch 19/20, Loss: 4.2134\n",
      "Epoch 20/20, Loss: 4.1573\n",
      "Prediction saved to: C:\\Users\\ISU\\cv_dataset\\slice_data\\segmented_mask.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nibabel as nib\n",
    "import scipy.ndimage\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import nrrd  # Library for handling .nrrd files\n",
    "\n",
    "# Define configuration parameters\n",
    "INPUT_SHAPE = (128, 128, 64)  # The target shape after resizing\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Base directories\n",
    "BASE_DIR = r\"C:\\Users\\ISU\\cv_dataset\\os_dataset\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "# Updated path for testing with your .nrrd file\n",
    "PATIENT_IMAGE_PATH = r\"C:\\Users\\ISU\\cv_dataset\\slice_data\\4 Unnamed Series_1.nrrd\"\n",
    "OUTPUT_MASK_PATH = r\"C:\\Users\\ISU\\cv_dataset\\slice_data\\segmented_mask.nii.gz\"\n",
    "\n",
    "# Create Dataset class to load images and masks\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def resize_3d(self, img, shape):\n",
    "        zoom_factors = [shape[i] / img.shape[i] for i in range(3)]\n",
    "        return scipy.ndimage.zoom(img, zoom_factors, order=1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.image_paths[idx]).get_fdata()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # Normalize the image\n",
    "        img = self.resize_3d(img, INPUT_SHAPE)\n",
    "        img = torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        if self.mask_paths:\n",
    "            mask = nib.load(self.mask_paths[idx]).get_fdata()\n",
    "            mask = self.resize_3d(mask, INPUT_SHAPE)\n",
    "            \n",
    "            # Convert mask to binary (0 or 1)\n",
    "            mask = (mask > 0).astype(np.float32)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "            \n",
    "            return img, mask\n",
    "        return img\n",
    "\n",
    "# Get all image and mask paths for training and validation\n",
    "train_img_paths = glob.glob(os.path.join(TRAIN_DIR, \"images\", \"*.nii\"))\n",
    "train_mask_paths = glob.glob(os.path.join(TRAIN_DIR, \"masks\", \"*.nii\"))\n",
    "val_img_paths = glob.glob(os.path.join(VAL_DIR, \"images\", \"*.nii\"))\n",
    "val_mask_paths = glob.glob(os.path.join(VAL_DIR, \"masks\", \"*.nii\"))\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = HeartDataset(train_img_paths, train_mask_paths)\n",
    "val_dataset = HeartDataset(val_img_paths, val_mask_paths)\n",
    "\n",
    "# Create DataLoader objects for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define and train the model (UNet)\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet3D, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv3d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(64, out_channels, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.upconv = nn.ConvTranspose3d(64, 64, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        dec = self.upconv(enc)\n",
    "        dec = self.decoder(dec)\n",
    "        return dec\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = UNet3D(in_channels=1, out_channels=1).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), os.path.join(BASE_DIR, \"unet_heart_segmentation_3d.pth\"))\n",
    "\n",
    "# --- Inference on Patient Image ---\n",
    "def predict_patient_image(model, patient_image_path, output_mask_path, device):\n",
    "    model.eval()\n",
    "    # Load and preprocess patient image\n",
    "    img, _ = nrrd.read(patient_image_path)  # Read .nrrd file\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize the image\n",
    "    img = scipy.ndimage.zoom(img, [float(dim) / img.shape[i] for i, dim in enumerate(INPUT_SHAPE)])\n",
    "    img = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)  # Add channel dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict segmentation mask\n",
    "        pred = model(img)\n",
    "        pred = (pred > 0.5).float().cpu().numpy().squeeze()  # Apply threshold\n",
    "\n",
    "    # Save the predicted mask as a NIfTI image\n",
    "    pred_img = nib.Nifti1Image(pred, np.eye(4))\n",
    "    nib.save(pred_img, output_mask_path)\n",
    "    print(f\"Prediction saved to: {output_mask_path}\")\n",
    "\n",
    "# Predict on the single patient image\n",
    "predict_patient_image(model, PATIENT_IMAGE_PATH, OUTPUT_MASK_PATH, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe0dc1-8f93-4a85-8706-defa19fac4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
